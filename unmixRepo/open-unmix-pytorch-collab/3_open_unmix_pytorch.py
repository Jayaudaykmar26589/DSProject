# -*- coding: utf-8 -*-
"""3 - Open Unmix - Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ

# Open-Unmix PyTorch

![](https://sisec18.unmix.app/static/img/hero_header.4f28952.svg)

__Open-Unmix__ is a deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists. This notebook provides easy access to pre-trained models that allow users to separate pop music into four stems: __vocals__, __drums__, __bass__ and the remaining __other__ instruments. The models were trained on the [MUSDB18](https://sigsep.github.io/datasets/musdb.html) dataset.

__Related Projects:__ open-unmix-pytorch | [open-unmix-nnabla](https://github.com/sigsep/open-unmix-nnabla) | [open-unmix-tensorflow](https://github.com/sigsep/open-unmix-tensorflow) | [musdb](https://github.com/sigsep/sigsep-mus-db) | [museval](https://github.com/sigsep/sigsep-mus-eval) | [norbert](https://github.com/sigsep/norbert)

## The Model

_Open-Unmix_ is based on a three-layer bidirectional deep LSTM. The model learns to predict the magnitude spectrogram of a target, like _vocals_, from the magnitude spectrogram of a mixture input. Internally, the prediction is obtained by applying a mask on the input. The model is optimized in the magnitude domain using mean squared error and the actual separation is done in a post-processing step involving a multichannel wiener filter implemented using [norbert](https://github.com/sigsep/norbert). To perform separation into multiple sources, multiple models are trained for each particular target. While this makes the training less comfortable, it allows great flexibility to customize the training data for each target source.

## How to run this notebook

We provide two pre-trained models:

* __`umxhq` (default)__  trained on [MUSDB18-HQ](https://sigsep.github.io/datasets/musdb.html#uncompressed-wav) which comprises the same tracks as in MUSDB18 but un-compressed which yield in a full bandwidth of 22050 Hz.

  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3267291.svg)](https://doi.org/10.5281/zenodo.3267291)

* __`umx`__ is trained on the regular [MUSDB18](https://sigsep.github.io/datasets/musdb.html#compressed-stems) which is bandwidth limited to 16 kHz do to AAC compression. This model should be used for comparison with other (older) methods for evaluation in [SiSEC18](sisec18.unmix.app).

  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3340804.svg)](https://doi.org/10.5281/zenodo.3340804)

## Performinng separation 

At the core of the separation is the `separate` function which takes a numpy audio array as input (the mixture) and separates into `targets` number of stems.

```python
def separate(
    audio,
    targets,
    model_name='umxhq',
    niter=1, softmask=False, alpha=1.0,
    residual_model=False, device='cpu'
):
    '''
    Performing the separation on audio input

    Parameters
    ----------
    audio: np.ndarray [shape=(nb_samples, nb_channels, nb_timesteps)]
        mixture audio

    targets: list of str
        a list of the separation targets.
        Note that for each target a separate model is expected
        to be loaded.

    model_name: str
        name of torchhub model or path to model folder, defaults to `umxhq`

    niter: int
         Number of EM steps for refining initial estimates in a
         post-processing stage, defaults to 1.

    softmask: boolean
        if activated, then the initial estimates for the sources will
        be obtained through a ratio mask of the mixture STFT, and not
        by using the default behavior of reconstructing waveforms
        by using the mixture phase, defaults to False

    alpha: float
        changes the exponent to use for building ratio masks, defaults to 1.0

    residual_model: boolean
        computes a residual target, for custom separation scenarios
        when not all targets are available, defaults to False

    device: str
        set torch device. Defaults to `cpu`.

    Returns
    -------
    estimates: `dict` [`str`, `np.ndarray`]
        dictionary of all restimates as performed by the separation model.

    '''
```
The model can be specified using `model_name` parameter. Both models `umx` and `umxhq` are downloaded automatically. The other parameters are sugggested to set to the default values if not suggested otherwise in the following examples.

### Colab Limitations 

* The disk and RAM is limited in colab. Loading the four separation models `vocals`, `drums`, `bass` and `other` is already using 400 MB of disk and RAM. 
* A major step in the separation is the post-processing, contolled by the parameters `niter`. For faster inference (at the expense of separation quality) it is adviced to use `niter=0`.
* Another way to prevent colab from crashing is to only perform separation on smaller excerpts. In the following examples we privide a way to set the start and stop duration of the audio being separated. We suggest __not to separate segements of longer than 30s__.

# Installation and Imports (RUN THESE CELLS FIRST)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install musdb
# !pip install norbert
# !pip install librosa
# !pip install youtube-dl
# !git clone https://github.com/sigsep/open-unmix-pytorch.git

import torch
import numpy as np
import scipy
import librosa
import youtube_dl
import os
import soundfile as sf
from google.colab import files
from IPython.display import Audio, display

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")

# Commented out IPython magic to ensure Python compatibility.
# %cd open-unmix-pytorch/
import test

"""# Separate MUSDB18 tracks

Get a musdb18 7 second preview track
"""

import musdb
mus = musdb.DB(download=True, subsets='test')

track = mus[49]
print(track.name)
display(Audio(track.audio.T, rate=track.rate))

"""###Apply separation into four stems

open-unmix is auto-downloading a model for each available target:

* vocals
* drums
* bass
* other
"""

estimates = test.separate(
    audio=track.audio, 
    targets=['vocals', 'drums', 'bass', 'other'], 
    residual_model=False,
    niter=1,
    device=device
)
for target, estimate in estimates.items():
    print(target)
    display(Audio(estimate.T, rate=track.rate))

"""### Apply separation into vocals/accompaniment

Even open-unmix does not provide a separate model for the accompaniment, we can use the spectral `residual` model in the post-processing to force a linear sum of all separated sources - e.g. this can be used for vocal/accompaniment separation. Note, that the sepearation performance is decreased when using the residual model.
"""

estimates = test.separate(
    audio=track.audio, 
    targets=['vocals'], 
    residual_model=True,
    device=device
)
for target, estimate in estimates.items():
    print(target)
    display(Audio(estimate.T, rate=track.rate))

"""Another way to achive vocal/accompanimnet separation is to sepearate into four stems and sum up the non-vocal stems."""

estimates = test.separate(
    audio=track.audio, 
    targets=['vocals', 'drums', 'bass', 'other'], 
    residual_model=True,
    device=device
)
print('vocals')
display(Audio(estimates['vocals'].T, rate=track.rate))
acc = np.sum(
    [audio for target, audio in estimates.items() if not target=='vocals'],
    axis=0
)
print('accompaniment')
display(Audio(acc.T, rate=track.rate))

"""# Separate Youtube Video"""

from IPython.display import HTML
url = "xwtdhWltSIg" #@param {type:"string"}
start = 60 #@param {type:"number"}
stop = 90 #@param {type:"number"}
embed_url = "https://www.youtube.com/embed/%s?rel=0&start=%d&end=%d&amp;controls=0&amp;showinfo=0" % (url, start, stop)
HTML('<iframe width="560" height="315" src=' + embed_url + 'frameborder="0" allowfullscreen></iframe>')

def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading...')


ydl_opts = {
    'format': 'bestaudio/best',
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'wav',
        'preferredquality': '44100',
    }],
    'outtmpl': '%(title)s.wav',
    'progress_hooks': [my_hook],
}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    info = ydl.extract_info(url, download=False)
    status = ydl.download([url])

audio, rate = librosa.load(info.get('title', None) + '.wav', sr=44100, mono=False)
audio = audio[:, start*rate:stop*rate]
print(audio.shape)
display(Audio(audio, rate=rate))
estimates = test.separate(audio=audio.T, targets=['vocals', 'drums', 'bass', 'other'], device=device, residual_model=False)
for target, estimate in estimates.items():
    print(target)
    display(Audio(estimate.T, rate=rate))

"""Download separations"""

for target, estimate in estimates.items():
    sf.write(target + '.wav', estimate, 44100, subtype='PCM_16')
    files.download(target + '.wav')

"""# Separate from uploaded file"""

from google.colab import files
uploaded = files.upload()

start = 0 #@param {type:"number"}
stop = 15 #@param {type:"number"}
audio, rate = librosa.load(
    list(uploaded.keys())[0],
    sr=44100,
    offset=start,
    duration=stop-start,
    mono=False
)
display(Audio(audio, rate=rate))
estimates = test.separate(
    audio=audio.T,
    targets=['vocals', 'drums', 'bass', 'other'],
    residual_model=False,
    device=device,
    niter=1
)
for target, estimate in estimates.items():
    print(target)
    display(Audio(estimate.T, rate=rate))

"""# Export estimates

After separation, you can save the results as wav files or STEMs.

## Download Separations to disk
"""

for target, estimate in estimates.items():
    sf.write(target + '.wav', estimate, 44100, subtype='PCM_16')
    files.download(target + '.wav')

"""## Encode to STEMS format"""

import stempeg
S = np.array([array for array in estimates.values()])
stempeg.write_stems(S, "umx.stem.m4a", rate=44100)
files.download("umx.stem.m4a")